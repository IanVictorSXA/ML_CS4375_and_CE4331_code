{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "5.Implement SVM and Perceptrons from Scratch (5 points) Use the simple dataset (using makeblobs function) that we used in the SVM demo (https://colab.research.google.com/drive/1z368aCHvKDy92E0dJXtQnTFgF3O1HOWo). Implement the Perceptron Loss and Hinge Loss functions. Next, implement a simple subgradient descent algorithm and optimize the perceptron and hinge losses. Compare the solutions obtained by the two."
      ],
      "metadata": {
        "id": "mWVpgp5gMOoo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fm8F2o0dNUGJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# My implementation of subgradient descend\n",
        "class subgradient_descent:\n",
        "  def __init__(self, learning_rate, epochs):\n",
        "    self.epochs = epochs\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "\n",
        "  def fit(self, x_train, y_train):\n",
        "    self.w = np.random.randn(x_train.shape[1])\n",
        "    self.b = np.random.randn()\n",
        "    for i in range(self.epochs):\n",
        "      y_pred = self.predict(x_train)\n",
        "      arr = np.where(-y_pred*y_train > 0, 1, 0)\n",
        "      self.w += self.learning_rate*np.dot(x_train.T * arr,y_train)\n",
        "      self.b += self.learning_rate*np.sum(y_train * arr)\n",
        "\n",
        "  def predict(self, x):\n",
        "    return np.dot(x, self.w) + self.b\n",
        "\n",
        "# Hinge loss function\n",
        "def hinge_loss(y_true, y_pred):\n",
        "    return np.sum(np.maximum(0, 1 - y_pred*y_true))\n",
        "\n",
        "# Perceptron loss function\n",
        "def perceptron_loss(y_true, y_pred):\n",
        "    return np.sum(np.maximum(0, -y_pred*y_true))"
      ],
      "metadata": {
        "id": "0in-0nmVRenR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied from professor's code\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "\n",
        "y = np.where(y == 0, -1, 1)"
      ],
      "metadata": {
        "id": "k8vGk9WrNn-o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating my subgradient descent object\n",
        "sub_des = subgradient_descent(learning_rate=0.1, epochs=100)\n",
        "\n",
        "# Training\n",
        "sub_des.fit(X, y)\n",
        "\n",
        "# Printing results\n",
        "print(\"Hinge loss:\",hinge_loss(y, sub_des.predict(X)))\n",
        "print(\"Perceptron loss:\", perceptron_loss(y, sub_des.predict(X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZNfVf_qdYsw",
        "outputId": "ff98230f-544f-4f53-f969-75cfff2d28de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hinge loss: 0.1919227093170247\n",
            "Perceptron loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "Using subgradient descent for optimzing the hinge and perceptron loss function with the hyperparameters learning rate = 0.1 and epochs = 100, the result for each loss function is very low even with a few number of epochs. When tuning the hyperparameters, the hinge loss always yields a value greater than or equal to the perceptron loss as it was showed in class."
      ],
      "metadata": {
        "id": "PD3xLJQZeoY5"
      }
    }
  ]
}