{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "2.Implement Logistic Regression (8 points): Implement a regularized Logistic Regression from scratch. Pick any dataset of your choice for this (you can use the UCI Machine Learning repository: https://archive.ics.uci.edu/ml/index.php or you can use a dataset we used in the past). Please verify the following:\n",
        "\n",
        "• First, start with the regular feature set with regularization = 0. Determine if you are underfitting ot overfitting.\n",
        "\n",
        "• If you are not overfitting, create polynomial features the way we did in the Linear Regression demo\n",
        "\n",
        "• Again, train a logistic regression with zero regularization and vary the polynomial degree till you overfit on the train data.\n",
        "\n",
        "• Once you overfit on the train data, start adding regularization. What happens with regularization? Can you reduce the overfitting with regularization?"
      ],
      "metadata": {
        "id": "ieQFKY1UPLU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwLSMAVNA49f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my logistic regression implementation\n",
        "class LogisticRegression:\n",
        "  def __init__(self, lr, iterations, C=1, penalty=None):\n",
        "    \"\"\" lr: learning rate\n",
        "        iterations: number of epochs\n",
        "        C: weight of regularization\n",
        "        penalty: type of regularization (l1, l2, None)\"\"\"\n",
        "    self.penalties = {None: self.none_gradient, \"l1\" : self.l1_gradient, \"l2\" : self.l2_gradient}\n",
        "    self.lr = lr\n",
        "    self.iterations = iterations\n",
        "    self.C = C\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "    self.penalty = self.penalties[penalty]\n",
        "\n",
        "  # fit function\n",
        "  def fit(self, X, y):\n",
        "    self.weights = np.random.rand(X.shape[1])\n",
        "    self.bias = np.array([0.0])\n",
        "    Xsum = X.sum(axis=0)\n",
        "    for i in range(self.iterations):\n",
        "      bias_gradient = 0\n",
        "      weight_gradient = 0\n",
        "      gradient = (self.predict(X) - y).sum()*self.lr\n",
        "\n",
        "      self.weights -= gradient * Xsum + self.penalty(self.weights)\n",
        "      self.bias -= gradient + self.penalty(self.bias)\n",
        "\n",
        "\n",
        "  # predict function\n",
        "  def predict(self, X):\n",
        "    return np.where(self.sigmoid(np.dot(X, self.weights) + self.bias) >= 0.5, 1, 0)\n",
        "\n",
        "  # gradient of L1 regularization (Lasso)\n",
        "  def l1_gradient(self, weights):\n",
        "    return self.C * np.sign(weights)\n",
        "\n",
        "  # gradent of L2 regularization (Ridge)\n",
        "  def l2_gradient(self, weights):\n",
        "    return self.C * weights\n",
        "\n",
        "  # in case penalty is None: do nothing\n",
        "  def none_gradient(self, weights):\n",
        "    return np.zeros(weights.shape)\n",
        "\n",
        "  # function of sigmoid\n",
        "  def sigmoid(self, z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# print train and test accuracies\n",
        "def print_results(y_train, y_test, y_pred_train, y_pred_test):\n",
        "  print(\"Train Accuracy: \", accuracy_score(y_train, y_pred_train))\n",
        "  print(\"Test Accuracy: \", accuracy_score(y_test, y_pred_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7CZJI0weDQz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from sklearn\n",
        "data = load_breast_cancer()\n",
        "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Normalizing data before giving it to logistic regression\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "\n",
        "# splitting into 2 datasets: 80% goes to train dataset and 20% goes to test dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# training logistic regression without any regularization\n",
        "lr = LogisticRegression(lr=0.00001, iterations=3000)\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = lr.predict(X_train)\n",
        "y_pred_test = lr.predict(X_test)\n",
        "\n",
        "print_results(y_train, y_test, y_pred_train, y_pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogKOd9swr_kj",
        "outputId": "f838efb8-3208-42bd-b8f3-aa833a7d2f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy:  0.7868131868131868\n",
            "Test Accuracy:  0.6929824561403509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model is overfitting on the training set!"
      ],
      "metadata": {
        "id": "luYChznpeAa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with L1 regularization (same learning rate but more iterations compared to no regularization model)\n",
        "lr = LogisticRegression(lr=0.00001, iterations=5000, C=1,penalty=\"l1\")\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = lr.predict(X_train)\n",
        "y_pred_test = lr.predict(X_test)\n",
        "\n",
        "print(\"L1 regularization: \", end=\"\")\n",
        "print_results(y_train, y_test, y_pred_train, y_pred_test)"
      ],
      "metadata": {
        "id": "HQ2WjOqxrrSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2acbdb8-0f9f-4e69-897a-5b779b4f1697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 regularization: Train Accuracy:  0.7736263736263737\n",
            "Test Accuracy:  0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with L2 regularization (same learning rate but more iterations compared to no regularization model)\n",
        "lr = LogisticRegression(lr=0.00001, iterations=5000, C=0.0001, penalty=\"l2\")\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = lr.predict(X_train)\n",
        "y_pred_test = lr.predict(X_test)\n",
        "\n",
        "print(\"L2 regularization: \", end=\"\")\n",
        "print_results(y_train, y_test, y_pred_train, y_pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sULq9oHhEGTo",
        "outputId": "99621119-722a-4e85-fbb5-0b54b7ce76b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 regularization: Train Accuracy:  0.7956043956043956\n",
            "Test Accuracy:  0.7017543859649122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  After trying many different values for C, I noticed that L1 regularization does a better job for my Logistic regression algorithm and that models with regularization take longer to train. L1 did remove the overfitting but L2 still overfits but the accuracies are a little better."
      ],
      "metadata": {
        "id": "cmlqiQWPeEch"
      }
    }
  ]
}